- https://martinfowler.com/bliki/Yagni.html
	- I love this figure
		- ![[Pasted image 20240308101335.png]] which cares about the cost
	- https://martinfowler.com/bliki/TechnicalDebt.html
- https://github.com/moneta-rb/moneta ofc Redis is not bad, but with freedom of K-V choices we can have many functionalities based on their specific design.
- The art of making expectation down to acceptance is called: "buffering"
- https://news.ycombinator.com/item?id=39635483
	- https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html "But here’s the key point: the gaming GPUs have similar performance to the data center GPUs that cost over 10x more! It would be great if we could use these 10x cheaper (but nearly as fast) cards to train large language models, but we can’t, **because they have much less memory**. The best currently available data center cards have 80GB RAM, whilst gaming cards max out at 24GB RAM. Since only the largest models produce the best results, creating the best models has been largely inaccessible to most people."
		- "And, of course, big tech companies are also full of brilliant people that solve hard problems. But this particular problem, training models with consumer GPUs, isn’t a problem they need to solve – they’ve already bought the big expensive GPUs! Many startups are also full of brilliant people that solve hard problems! But, as [Eric Ries explains](https://ltse.com/about/mission), “today’s financial market forces businesses to prioritize short-term gains over everything else”. It’s extremely hard for a startup to justify to investors why they’re spending their funds on open source software and public research."