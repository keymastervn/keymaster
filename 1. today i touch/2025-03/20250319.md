- https://en.wikipedia.org/wiki/Microsoft_Recall
	- "Recall's release immediately caused controversy, with experts warning that the feature could be a "disaster" for security and privacy,[[2]](https://en.wikipedia.org/wiki/Microsoft_Recall#cite_note-RecallVerge-2)Â especially as users were initially forced to have Recall enabled", well, it is a good idea for getting specialised training data
-  https://agentverse.ai/agents/details/agent1qvcqsyxsq7fpy9z2r0quvng5xnhhwn3vy7tmn5v0zwr4nlm7hcqrckcny9e/profile 
	- somehow, I feel like agentic platforms are just another no code solution, plus are ultra-expensive.
	- Attention is all "you" have, but "you" don't store them as training data; we are all making this mistake by chasing generic models. I am looking for an agent that can summarize my company workflows, break down a workflow into steps, then for each step finding good agents to work on that. It turns out I am "the alpha agent" because there is no such tool. And to think as an agent, I need "attention".
	- As an end user, agents are like multi personas and I don't like that. I think I should chat with just 1 thing, then that thing will call to specific agents to fetch/work on data. Comparing to RAG, it is a little bit slower in response, need to make some logic and use a tool call LLM and can even create a feature request tool to collect what users wildly want, can even personalise for them.
		- "I am a multi-purpose AI agent, what can I help you?" can be a good prompt
	- domain-specific microservice agent should be a protocol problem and is fully evil. But when it is done properly, we've just explored a brave new galaxy.
	- developer agent, are engineering tasks supporter. Do you ever think of SAP admin agent? You can even prompt it to run data extraction and feel good, but do you really want it that evil-prone way?
		- https://agentverse.ai/agents/details/agent1qtjamlz8u3nrlmsx7c70vvysdzqknxr638aekn22rewr8aj4vwjrqnsyuxy/profile
- https://dagshub.com/blog/how-to-train-a-custom-llm-embedding-model/
	- Decoder-only transformers for embedding: currently, the main architecture used for embedding models is pre-trained bidirectional encoders or encoder-decoders such as BERT and T5. As opposed to decoder models like GPT and Llama, these models have not been designed to generate tokens but to encode their semantic content as a numerical vector."
	- https://www.reddit.com/r/LocalLLaMA/comments/19419yg/creating_your_own_embedding_model_to_boost/
	- https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/
- https://www.reddit.com/r/MachineLearning/comments/1fktvbj/p_swapping_embedding_models_for_an_llm/