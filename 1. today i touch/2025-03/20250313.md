- https://www.reddit.com/r/ClaudeAI/comments/1j7k7qz/why_context_size_is_misunderstood_and_how_models/
	- https://arxiv.org/html/2502.05167v1
	- "In truth, majority of models fall of hard after 8k and 16k context. All of them do after 32k.  2m context claim by Gemini is complete BS."
	- "Honestly half of what makes LLM's work are clever workaround instead of genuine breakthroughs. I'll take what I can get lol"
- https://ai.google.dev/gemma Gemma 3, 128k context length
	- https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama