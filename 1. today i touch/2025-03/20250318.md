- https://kili-technology.com/large-language-models-llms/a-guide-to-aligning-large-language-models-llms-through-data
	- https://kili-technology.com/large-language-models-llms/behind-the-scenes:-evaluating-llm-red-teaming-techniques-and-categories
		- alignment: red teaming techniques to prevent bad LLMs
		- ![[Pasted image 20250318104439.png|300]]
		- this is an LLM testing company
	- ![[Pasted image 20250318104614.png|300]]
	- https://kili-technology.com/large-language-models-llms/a-guide-to-using-small-language-models
		- "In [a study by Anthropic](https://arxiv.org/abs/2204.05862), it was found that large language models can receive an alignment bonus, wherein model performance improves after RLHF. In the meantime, small language models suffered from an alignment tax, which means the model's performance was reduced after RLHF."
		- "Organizations should implement a [robust evaluation and monitoring strategy](https://kili-technology.com/large-language-models-llms/a-guide-to-rag-evaluation-and-monitoring-2024) to maximize the effectiveness of small language models paired with RAG. This strategy is essential to ensure that the SLM and RAG stack produces the desired outputs and meets specific operational requirements."
		- "Adhering to brand tone and identity: When writing large amounts of content, a large language model can be useful to generate content that follows branding and company guidelines at scale. In this case, machine learning (ML) teams can use a combination of techniques mentioned in earlier examples to develop an aligned model that adheres to a company's branding and tone guidelines."
-  https://huggingface.co/datasets/tiiuae/falcon-refinedweb hmm, in the famous youtube video about LLM that you perhaps know
	- https://commoncrawl.org/
	- https://youtu.be/20wbA_ijjmg?si=aqR9_c7Isv4qjwlJ&t=1270
	- https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
- https://arxiv.org/abs/2205.14135 FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. On scaling Attention performance.
	- https://viblo.asia/p/llm-101-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness-yZjJY3jXVOE Flash Attention, engineering-only, Tiling và Recomputation
- https://mistral.ai/news/mistral-small-3-1
	- Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.
	- Low-latency function calling: Capable of rapid function execution within automated or agentic workflows
	- Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model.
	- https://www.reddit.com/r/LocalLLaMA/comments/1jdgqcj/new_mistral_just_dropped/
- https://sloanreview.mit.edu/article/intelligent-choices-reshape-decision-making-and-productivity/
- https://huggingface.co/papers/2503.11576 SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion (256M)
	- https://arxiv.org/html/2503.11576v1