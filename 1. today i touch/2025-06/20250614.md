- https://www.seangoedecke.com/point-by-point-considered-harmful/ "But in cases of disagreement - especially disagreement about technical topics, like planning a software feature - it’s not always wise to go point-by-point."
- https://www.anthropic.com/engineering/built-multi-agent-research-system
	- "**Tool design and selection are critical.** Agent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient—often, it’s strictly necessary. For instance, an agent searching the web for context that only exists in Slack is doomed from the start. With [MCP servers](https://modelcontextprotocol.io/introduction) that give the model access to external tools, this problem compounds, as agents encounter unseen tools with descriptions of wildly varying quality. We gave our agents explicit heuristics: for example, examine all available tools first, match tool usage to user intent, search the web for broad external exploration, or prefer specialized tools over generic ones. **Bad tool descriptions can send agents down completely wrong paths**, so each tool needs a **distinct purpose** and a **clear description**."
	- "Let agents improve themselves. We found that the Claude 4 models can be excellent prompt engineers. When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements. **We even created a tool-testing agent—when given a flawed MCP tool, it attempts to use the tool and then rewrites the tool description** to avoid failures. By testing the tool dozens of times, this agent found key nuances and bugs. This process for improving tool ergonomics resulted in a 40% decrease in task completion time for future agents using the new description, because they were able to avoid most mistakes." -> this is what I always think about
	- ![[Screenshot 2025-06-14 at 13.12.19.png]]
		- shit, this is the map I always want, https://www.anthropic.com/research/clio
	- https://newsletter.pragmaticengineer.com/p/cursor
- https://cognition.ai/blog/dont-build-multi-agents counter above Anthropic's article (Devin authors)
	- because of "**unreliability**", context overflow, memory issue
	- https://docs.crewai.com/concepts/agents#troubleshooting-common-issues
	- "As of June 2025, Claude Code is an example of an agent that spawns subtasks. However, it never does work in parallel with the subtask agent, and the subtask agent is usually only tasked with answering a question, not writing any code. Why? The subtask agent lacks context from the main agent that would otherwise be needed to do anything beyond answering a well-defined question. And if they were to run multiple parallel subagents, they might give conflicting responses, resulting in the reliability issues we saw with our earlier examples of agents. **The benefit of having a subagent in this case is that all the subagent’s investigative work does not need to remain in the history of the main agent**, allowing for longer traces before running out of context. The designers of Claude Code took a purposefully simple approach."
	- Edit-Apply model
		- "In 2024, many models were really bad at editing code. A common practice among coding agents, IDEs, app builders, etc. (including Devin) was to use an “edit apply model.” The key idea was that it was actually more reliable to get a small model to rewrite your entire file, given a markdown explanation of the changes you wanted, than to get a large model to output a properly formatted diff. So, builders had the large models output markdown explanations of code edits and then fed these markdown explanations to small models to actually rewrite the files. However, these systems would still be very faulty. Often times, for example, the small model would misinterpret the instructions of the large model and make an incorrect edit due to the most slight ambiguities in the instructions. Today, the edit decision-making and applying are more often done by a single model in one action."