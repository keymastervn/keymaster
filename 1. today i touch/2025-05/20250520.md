- https://platform.openai.com/docs/guides/evals-design
- http://newsletter.pragmaticengineer.com/p/ai-engineering-with-chip-huyen
	- https://www.youtube.com/watch?v=98o_L3jlixw
	- **1. “AI engineering” feels closer to software engineering than to ML engineering.** The term itself is very new, and “AI engineering” evolved from ML engineering. A big difference is that thanks to LLMs being easy to use (both via APIs, and locally) “AI engineering” is much more about building a product first – and later on, getting around to tweaking the model itself. ML engineering was the other way around: spend a lot of time building a model, and then build an application on it.
	- **2. To get good at AI engineering, focus on the basics.** Understand what an LLM is (and how it works), how to evaluate them, how to use RAG, what finetuning is, and how to optimize inference. All of these techniques are foundational, and will remain important in a few years’ time as well. Chip’s book [AI Engineering](https://learning.oreilly.com/library/view/ai-engineering/9781098166298/) covers all these topics.
	- **3. “The more we want to not miss out on things, the more things we will miss.”** When I asked Chip about what she thinks about the fear of missing out (FOMO) across AI: she said it’s ironic that when you are worried about missing out, and keep up with everything new – you stay shallow! Chip doesn’t read news: she chooses to focus on deep research or learning instead. If there’s something that is important today: it will be important next week, after she’s done finishing learning whatever she was focused on.
	- **4. Solving the problem is more important than using the latest AI tools**. Amusingly, a lot of teams miss this part: and they build overcompliacated AI solutions that do practically nothing for the business.
	- AI can enable software engineers to build much more complex software.
	- AI will not eliminate software engineering because it can only automate part of the software engineering process. The need for precision in software development requires someone who understands the nuances of code.
	- https://dev.botframework.com/
		- no code agent platform ![[Screenshot 2025-05-20 at 21.02.47.png]]
	- ![[Screenshot 2025-05-20 at 21.09.40.png]] yes!
- https://lilianweng.github.io/posts/2024-11-28-reward-hacking/#lets-define-reward-hacking
	- "They listed **reward hacking** as one of the key AI safety problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high reward through undesired behavior. **Specification gaming** ([Krakovna et al. 2020](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)) is a similar concept, defined as a behavior that satisfies the literal specification of an objective but not achieving the desired results." -> human, too, reward is a source of evil intents
	- "**Reward Tampering** ([Everitt et al. 2019](https://arxiv.org/abs/1908.04734)) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal"
	- Why does Reward Hacking Exist?
		- [**Goodhart’s Law**](https://en.wikipedia.org/wiki/Goodhart%27s_law) states that _“When a measure becomes a target, it ceases to be a good measure”_. The intuition is that a good metric can become corrupted once significant pressure is applied to optimize it. It is challenging to specify a 100% accurate reward objective and any _proxy_ suffers the risk of being hacked, as RL algorithm exploits any small imperfection in the reward function definition. [Garrabrant (2017)](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) categorized Goodhart’s law into 4 variants: Regressional, Extremal, Causal, Adversarial
	- Well, they deeply teach me some concepts about hacking the system.