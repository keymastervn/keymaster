- https://en.wikipedia.org/wiki/Rules_of_engagement "They provide authorization for and/or limits on, among other things, the use of force and the employment of certain specific capabilities"
- https://github.com/rails/activeresource/blob/main/lib/active_resource/active_job_serializer.rb see this serialize, deserialize, we may need this one but not through RESTful requests but RPC requests.
- https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/ "Standard k-NN search methods compute similarity using a brute-force approach that measures the nearest distance between a query and a number of points, which produces exact results. This works well in many applications. However, in the case of extremely large datasets with high dimensionality, this creates a scaling problem that reduces the efficiency of the search"
	- k-NN has a performance limitation, ANN is the work of multiple dimensions storing vectors. When accepting less accuracy (if not saying: absolute accuracy), ANN outperforms in latency and SHOULD BE USED in general.
		- k-NN works, because we limited its context in a specific domain, aka 1 dimension.
		- The vast number of dimensions visualizes our real-life knowledge of concepts
			- https://supabase.com/blog/openai-embeddings-postgres-vector "Compared to our 2-dimensional example above, their latest embedding model `text-embedding-ada-002` will output 1536 dimensions"
			- https://github.com/pgvector/pgvector-ruby/blob/master/examples/openai_embeddings.rb we can start getting data based off `1536 dimensions` in `text-embedding-ada-002`. Whether it has 10K or 1M records, just fetch those embeddings and insert into your DB for native semantics search.
			- https://github.com/pgvector/pgvector v0.5 support HNSW index a month ago, so it is closer to OpenSearch, very close, very approachable.
			- "An HNSW index creates a multilayer graph. It has slower build times and uses more memory than IVFFlat, but has better query performance (in terms of speed-recall tradeoff). There’s no training step like IVFFlat, so the index can be created without any data in the table."
			- https://github.com/pgvector/pgvector#approximate-search
			- From Holistics: Postgres is enough; persistence is moving to disk as being faster and cheaper
	- "codecs handle the storage and retrieval of indexes"
	- "...dimension count of up to 16,000 for the `nmslib` and `faiss` engines, as set by the dimension mapping parameter. The maximum dimension count for the `Lucene` library is 1,024". "Lucene engine with a Hierarchical Navigable Small World (HNSW) algorithm (k-NN plugin versions 2.4 and later)"
		- https://www.crunchydata.com/blog/pgvector-performance-for-developers