- Sad, backfilling 77 million records is not confortable within a migration
	- https://world.hey.com/paulosilva/ruby-on-rails-backfilling-data-136adb66
		- Independently of which option you choose, there are three things to take into consideration to backfill data safely: **batching**, **throttling** and **running outside a transaction**. 
			- learnt it by heart :point-up:
	- Sad, sequel does not support batch operation
		- https://devhints.io/sequel#:~:text=DELETE%20QUERY%0Aafter_destroy-,Schema,-class%20Deal%20%3C this "schema" in-model idea is very much model as a set of configurations and concerns, not ActiveRecord pattern (OOP), but it counters our conventional wisdom due to Rails
	- https://engineering.ezcater.com/migrate-transform-and-backfill-data-with-zero-downtime
	- https://blog.noredink.com/post/170966460333/backfilling-more-faster, 250mil records, but this guy is using multi threads that might spike the DB IO, hmm
	- https://gist.github.com/adamrdavid/a6918223bf8b5d863ba8f41e11c47ecb Database do's and don'ts
	- https://www.honeybadger.io/blog/zero-downtime-migrations-of-large-databases-using-rails-postgres-and-redis/ 500M records problem, using the application logic to check-and-migrate / or simply flow control null cases
- https://thenewstack.io/the-architects-guide-to-data-and-file-formats Parquet, ORC, Avro, Arrow, Protobuf, Thrift and MessagePack. What are they and how to choose the right one?