- https://en.wikipedia.org/wiki/How_to_Solve_It#Heuristics
- https://www.anthropic.com/research/building-effective-agents prime knowledge
	- "One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good agent-computer interfaces (ACI)"
- ![[Pasted image 20250225163132.png]] cre: Alex Xu - bytebytego
- Can a bot/agent self-modify code of a handler to fulfill its need? 
- https://venturebeat.com/ai/what-you-need-to-know-about-sakana-ai-the-new-startup-from-a-transformer-paper-co-author/
	- https://sakana.ai/transformer-squared/ source
		- https://www.anthropic.com/research#interpretability
		- https://www.goodfire.ai/
	- "Last month saw one of the biggest defections: Llion Jones, one of the co-authors on the seminal 2017 research paper “[Attention Is All You Need](https://arxiv.org/abs/1706.03762),” which kickstarted the generative AI revolution by developing the architecture of the transformers used in leading large language models (LLMs), [announced he’d left Google to found a new startup.](https://www.bloomberg.com/news/articles/2023-07-11/ai-researcher-who-helped-write-landmark-paper-is-leaving-google?in_source=embedded-checkout-banner)"
	- "“Ants move around and dynamically form a bridge by themselves, which might not be the strongest bridge, but they can do it right away and adapt to the environments,” Ha told Bloomberg in an interview. “I think this sort of adaptation is one of the very powerful concepts that we see in natural algorithms.”"
		- I see this beautiful, not just powerful. It skips existing laws by using one simple law: evolutionary; inefficiency but can be an answer to everything.
	- "The cofounders [told CNBC](https://www.cnbc.com/2023/08/17/transformer-co-author-llion-jones-leaves-google-for-startup-sakana-ai.html) they believed Google’s **focus on a single type of generative AI technology, large language models, was a mistake** “because that’s quite a **restrictive** framework,” Jones said. The cofounders said they’ve been in talks about using LLMs, but haven’t made a final determination. However, Ha did not write them off: “I would be surprised if language models were not part of the future,” he told CNBC."
	- https://huggingface.co/docs/diffusers/training/lora
		- "LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share. LoRA can also be combined with other training techniques like DreamBooth to speedup training."