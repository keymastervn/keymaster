- https://www.morningstar.com/stocks/updating-gold-price-forecasts-miner-fair-value-estimates
	- "We update our gold price forecasts. Based on the futures curve, we now assume gold averages $3,170 per ounce from 2025 to 2027, up from around $2,810. **We also raise our assumed midcycle gold price to $2,000 per ounce from 2029, up from $1,820.**"
- https://www.reddit.com/r/singularity/comments/1k4wpc2/geoffrey_hinton_humans_arent_reasoning_machines/
	- Geoffrey Hinton: ‘Humans aren’t reasoning machines. We’re analogy machines, thinking by resonance, not deduction.’
	- Well, AI will learn sentiment, soon.
- https://www.reddit.com/r/LocalLLaMA/comments/1k2kl84/gemma_3_27b_is_underrated_af_its_at_11_at_lmarena/
- https://www.reddit.com/r/LocalLLaMA/comments/1k25876/comment/mntd9p9/
	- ```
	QAT is not "training after quantization".
	The flow is not
	
	pretrain --> quantize --> QAT --> final-QAT-model
	
	it's more like
	
	pretrain --> QAT --> quantize --> final-QAT-model-quantized
	
	They explain this a bit in the blog post
	"QAT incorporates the quantization process **during training**. QAT **simulates** low-precision operations **during training** to allow quantization with less degradation **afterwards** for smaller, faster models while maintaining accuracy. "
	
	emphasis mine.
	
	It's a very minute detail, but worth mentioning because it's very interesting how it works.
	
	To be extra extra clear, the output of QAT is not the quantized model. It is the full-precision (or half I guess at bf16) model that has been trained with an extra step that simulates quantization. So, when the real quantization finally happens after QAT, there is less information lost because it had some quantization-like operations simulated during its original training.```
- https://github.com/nari-labs/dia text-to-dialog inference
- https://cookbook.openai.com/examples/gpt4-1_prompting_guide
	- "As mentioned already, developers can optionally prompt agents built with GPT-4.1 to plan and reflect between tool calls, instead of silently calling tools in an unbroken sequence. GPT-4.1 is not a reasoning model - meaning that it does not produce an internal chain of thought before answering - but in the prompt, a developer can induce the model to produce an explicit, step-by-step plan by using any variant of the Planning prompt component shown above. This can be thought of as the model “thinking out loud.” In our experimentation with the SWE-bench Verified agentic task, inducing explicit planning increased the pass rate by 4%."
	- https://github.com/togethercomputer/together-cookbook?tab=readme-ov-file#cookbooks