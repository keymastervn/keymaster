- On ATS rate limit issue: coming from `lib/gmail/mark_as_read.rb`, fixed window or sliding window didn't help.
	- quota: https://developers.google.com/gmail/api/reference/quota
		- Per user rate limit => 250 quota units per user per second,Â _moving average_Â (allows short bursts).
		- Per method
			- messages.modify => 5 per sec
	- Why a fixed window rate limiter with 30 rqs did not help?
		- `rescue RateLimiter::Errors::ExceedLimitError => e` => this is the error, over the rate limit, it simply retries at worker level with a random jitter. Not actually failed but it is rescheduling with 12K sentry âœ…
		- Checked dashboard, around 300 per min for just 3 min, 49 per 5s. Curious: why we are going with this design?
	- https://www.reddit.com/r/ruby/comments/15342sc/why_adaptive_rate_limiting_is_a_gamechanger/
		- adaptive rate limiting?
			- https://www.linkedin.com/pulse/adaptive-throttling-maximize-throughput-microservices-mark-grand/ upstream throttling
				- "Begins **throttling** requests when the response time of downstream services increases **enough for its auto-scaling** or auto-healing mechanisms to notice." => this is one valid point towards the resiliency, people have been underestimated like GRPC deadline exceeded due to threadpool, what if we can add a clientside throttling here, waiting (ofc. communication) for the autoscaling of GRPC server to work? ðŸ’¡
			- https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581
		- https://github.com/earrrl/earrrl-ruby Estimated Average Recent Request Rate Limiter
			- "With EARRRL, if a user's estimated rate exceeds the rate limit, then they will be rate limited indefinitely" => not our use case, like a middle man it temporarily bans consumer, can be used for chatGPT requests.
		- https://blog.heroku.com/rate-throttle-api-client interesting!
			- https://brandur.org/rate-limiting Brandur is another good guy similar to Alex Xu
				- He spotted out two problems with a naive rate limiter: (1) short intense bursts, (2) separating good, ugly, bad clients
					- Leaky bucket is, perhaps an overflow treatment lead to autoscaling. Imagine a huge combinedlag of Kafka consumers
				- [Generic Cell Rate Algorithm](https://en.wikipedia.org/wiki/Generic_cell_rate_algorithm) tracking remaining limit, duration - cost to the current time. Then, based on the expected time, if another try comes => limit.
					- But this one is just another server-side rate limiting
					- https://g.co/bard/share/c4323cf4a8db Bard's explanation.
						- "Here's an analogy that might help a 12-year-old student understand GCRA. Imagine you're at a water park and there's a water slide that can only hold 10 people at a time. If 11 people try to get on the slide at the same time, the lifeguard will have to turn some people away. The lifeguard might randomly turn away one person, or they might turn away a few people. The goal of the lifeguard is to make sure that everyone has a fair chance of getting on the slide, even if the slide is very popular."
						- "The difference between GCRA and the simple rate limit example is that GCRA takes into account how many requests a user has made in the past. If a user has made a lot of requests in a short period of time, GCRA is more likely to delay or deny their requests. This is because GCRA is trying to prevent users from overloading the website or API."
- ![[Screenshot 2023-07-21 at 20.30.01.png]]
	- When I listen, I am lost too. fu*k
		- Still, I am too talkative, I just have been internalising my voice as a part of growing up: When you're in deep shit, it's best to keep mouth shut. But I am thinking more than an introvert and speak out more than an extrovert, in average.
	- It takes everyone 2 years to learn how to talk, but a whole like to learn how to keep silent